--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/parser.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/parser.py	(refactored)
@@ -6,8 +6,8 @@
 """ Multi-scale Brain Parcellator Commandline Parser
 """
 
-from info import __version__
-from info import __release_date__
+from .info import __version__
+from .info import __release_date__
 
 
 def get():
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/project.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/project.py	(refactored)
@@ -24,7 +24,7 @@
 
 import multiprocessing
 
-import ConfigParser
+import configparser
 
 from bids import BIDSLayout
 
@@ -106,7 +106,7 @@
     fmri_stage_names = List
     fmri_custom_last_stage = Str
     
-    number_of_cores = Enum(1, range(1, multiprocessing.cpu_count() + 1))
+    number_of_cores = Enum(1, list(range(1, multiprocessing.cpu_count() + 1)))
 
 
 def fix_dataset_directory_in_pickles(local_dir, mode='local', debug=False):
@@ -185,14 +185,14 @@
 
 
 def get_process_detail(project_info, section, detail):
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     # print('Loading config from file: %s' % project_info.config_file)
     config.read(project_info.config_file)
     return config.get(section, detail)
 
 
 def get_anat_process_detail(project_info, section, detail):
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     # print('Loading config from file: %s' % project_info.config_file)
     config.read(project_info.anat_config_file)
     res = None
@@ -204,34 +204,34 @@
 
 
 def get_dmri_process_detail(project_info, section, detail):
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     # print('Loading config from file: %s' % project_info.config_file)
     config.read(project_info.dmri_config_file)
     return config.get(section, detail)
 
 
 def get_fmri_process_detail(project_info, section, detail):
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     # print('Loading config from file: %s' % project_info.config_file)
     config.read(project_info.fmri_config_file)
     return config.get(section, detail)
 
 
 def anat_save_config(pipeline, config_path):
-    config = ConfigParser.RawConfigParser()
+    config = configparser.RawConfigParser()
     config.add_section('Global')
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     for key in global_keys:
         # if key != "subject" and key != "subjects":
         config.set('Global', key, getattr(pipeline.global_conf, key))
-    for stage in pipeline.stages.values():
+    for stage in list(pipeline.stages.values()):
         config.add_section(stage.name)
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             keyval = getattr(stage.config, key)
             if 'config' in key:  # subconfig
-                stage_sub_keys = [prop for prop in keyval.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(keyval.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     config.set(stage.name, key + '.' + sub_key, getattr(keyval, sub_key))
             else:
@@ -247,20 +247,20 @@
 
 
 def anat_load_config(pipeline, config_path):
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     config.read(config_path)
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     for key in global_keys:
         if key != "subject" and key != "subjects" and key != "subject_session" and key != "subject_sessions" and key != 'modalities':
             conf_value = config.get('Global', key)
             setattr(pipeline.global_conf, key, conf_value)
-    for stage in pipeline.stages.values():
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+    for stage in list(pipeline.stages.values()):
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             if 'config' in key:  # subconfig
                 sub_config = getattr(stage.config, key)
-                stage_sub_keys = [prop for prop in sub_config.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(sub_config.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     try:
                         conf_value = config.get(stage.name, key + '.' + sub_key)
@@ -287,20 +287,20 @@
 
 
 def dmri_save_config(pipeline, config_path):
-    config = ConfigParser.RawConfigParser()
+    config = configparser.RawConfigParser()
     config.add_section('Global')
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     for key in global_keys:
         # if key != "subject" and key != "subjects":
         config.set('Global', key, getattr(pipeline.global_conf, key))
-    for stage in pipeline.stages.values():
+    for stage in list(pipeline.stages.values()):
         config.add_section(stage.name)
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             keyval = getattr(stage.config, key)
             if 'config' in key:  # subconfig
-                stage_sub_keys = [prop for prop in keyval.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(keyval.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     config.set(stage.name, key + '.' + sub_key, getattr(keyval, sub_key))
             else:
@@ -316,20 +316,20 @@
 
 
 def dmri_load_config(pipeline, config_path):
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     config.read(config_path)
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     for key in global_keys:
         if key != "subject" and key != "subjects" and key != "subject_session" and key != "subject_sessions" and key != 'modalities':
             conf_value = config.get('Global', key)
             setattr(pipeline.global_conf, key, conf_value)
-    for stage in pipeline.stages.values():
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+    for stage in list(pipeline.stages.values()):
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             if 'config' in key:  # subconfig
                 sub_config = getattr(stage.config, key)
-                stage_sub_keys = [prop for prop in sub_config.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(sub_config.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     try:
                         conf_value = config.get(stage.name, key + '.' + sub_key)
@@ -355,20 +355,20 @@
 
 
 def fmri_save_config(pipeline, config_path):
-    config = ConfigParser.RawConfigParser()
+    config = configparser.RawConfigParser()
     config.add_section('Global')
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     for key in global_keys:
         # if key != "subject" and key != "subjects":
         config.set('Global', key, getattr(pipeline.global_conf, key))
-    for stage in pipeline.stages.values():
+    for stage in list(pipeline.stages.values()):
         config.add_section(stage.name)
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             keyval = getattr(stage.config, key)
             if 'config' in key:  # subconfig
-                stage_sub_keys = [prop for prop in keyval.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(keyval.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     config.set(stage.name, key + '.' + sub_key, getattr(keyval, sub_key))
             else:
@@ -384,20 +384,20 @@
 
 
 def fmri_load_config(pipeline, config_path):
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     config.read(config_path)
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     for key in global_keys:
         if key != "subject" and key != "subjects" and key != "subject_session" and key != "subject_sessions" and key != 'modalities':
             conf_value = config.get('Global', key)
             setattr(pipeline.global_conf, key, conf_value)
-    for stage in pipeline.stages.values():
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+    for stage in list(pipeline.stages.values()):
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             if 'config' in key:  # subconfig
                 sub_config = getattr(stage.config, key)
-                stage_sub_keys = [prop for prop in sub_config.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(sub_config.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     try:
                         conf_value = config.get(stage.name, key + '.' + sub_key)
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/bidsappmanager/gui.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/bidsappmanager/gui.py	(refactored)
@@ -232,7 +232,7 @@
     fmri_stage_names = List
     fmri_custom_last_stage = Str
     
-    number_of_cores = Enum(1, range(1, multiprocessing.cpu_count()))
+    number_of_cores = Enum(1, list(range(1, multiprocessing.cpu_count())))
     
     summary_view_button = Button('Pipeline processing summary')
     
@@ -544,7 +544,7 @@
     subjects = List(Str)
     
     # multiproc_number_of_cores = Int(1)
-    number_of_participants_processed_in_parallel = Enum(1, range(1, multiprocessing.cpu_count()))
+    number_of_participants_processed_in_parallel = Enum(1, list(range(1, multiprocessing.cpu_count())))
     
     # handler = Instance(project.CMP_BIDSAppWindowHandler)
     
@@ -1585,7 +1585,7 @@
             if self.anat_pipeline is not None:
                 print("> Anatomical pipeline output inspection")
                 self.anat_pipeline.view_mode = 'inspect_outputs_view'
-                for stage in self.anat_pipeline.stages.values():
+                for stage in list(self.anat_pipeline.stages.values()):
                     print("  ... Inspect stage {}".format(stage))
                     stage.define_inspect_outputs()
                     # print('Stage {}: {}'.format(stage.stage_dir, stage.inspect_outputs))
@@ -1595,7 +1595,7 @@
             if self.dmri_pipeline is not None:
                 print("> Diffusion pipeline output inspection")
                 self.dmri_pipeline.view_mode = 'inspect_outputs_view'
-                for stage in self.dmri_pipeline.stages.values():
+                for stage in list(self.dmri_pipeline.stages.values()):
                     print("  ... Inspect stage {}".format(stage))
                     stage.define_inspect_outputs()
                     # print('Stage {}: {}'.format(stage.stage_dir, stage.inspect_outputs))
@@ -1605,7 +1605,7 @@
             if self.fmri_pipeline is not None:
                 print("> fMRI pipeline output inspection")
                 self.fmri_pipeline.view_mode = 'inspect_outputs_view'
-                for stage in self.fmri_pipeline.stages.values():
+                for stage in list(self.fmri_pipeline.stages.values()):
                     print("  ... Inspect stage {}".format(stage))
                     stage.define_inspect_outputs()
                     # print('Stage {}: {}'.format(stage.stage_dir, stage.inspect_outputs))
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/bidsappmanager/project.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/bidsappmanager/project.py	(refactored)
@@ -26,7 +26,7 @@
 import string
 import gzip
 
-import ConfigParser
+import configparser
 from pyface.api import FileDialog, OK
 
 from bids import BIDSLayout
@@ -38,8 +38,8 @@
 from cmp.bidsappmanager.pipelines.diffusion import diffusion as Diffusion_pipeline
 from cmp.bidsappmanager.pipelines.anatomical import anatomical as Anatomical_pipeline
 
-import gui
-import core
+from . import gui
+from . import core
 
 
 # import CMP_MainWindow
@@ -274,14 +274,14 @@
 
 
 def get_process_detail(project_info, section, detail):
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     # print('Loading config from file: %s' % project_info.config_file)
     config.read(project_info.config_file)
     return config.get(section, detail)
 
 
 def get_anat_process_detail(project_info, section, detail):
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     # print('Loading config from file: %s' % project_info.config_file)
     config.read(project_info.anat_config_file)
     res = None
@@ -293,34 +293,34 @@
 
 
 def get_dmri_process_detail(project_info, section, detail):
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     # print('Loading config from file: %s' % project_info.config_file)
     config.read(project_info.dmri_config_file)
     return config.get(section, detail)
 
 
 def get_fmri_process_detail(project_info, section, detail):
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     # print('Loading config from file: %s' % project_info.config_file)
     config.read(project_info.fmri_config_file)
     return config.get(section, detail)
 
 
 def anat_save_config(pipeline, config_path):
-    config = ConfigParser.RawConfigParser()
+    config = configparser.RawConfigParser()
     config.add_section('Global')
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     for key in global_keys:
         # if key != "subject" and key != "subjects":
         config.set('Global', key, getattr(pipeline.global_conf, key))
-    for stage in pipeline.stages.values():
+    for stage in list(pipeline.stages.values()):
         config.add_section(stage.name)
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             keyval = getattr(stage.config, key)
             if 'config' in key:  # subconfig
-                stage_sub_keys = [prop for prop in keyval.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(keyval.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     config.set(stage.name, key + '.' + sub_key, getattr(keyval, sub_key))
             else:
@@ -335,27 +335,27 @@
 
 def anat_load_config(pipeline, config_path):
     print('>> Load anatomical config file : {}'.format(config_path))
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     
     datalad_is_available = is_tool('datalad')
     try:
         config.read(config_path)
-    except ConfigParser.MissingSectionHeaderError:
+    except configparser.MissingSectionHeaderError:
         print(
             '... error : file is a datalad git annex but it has not been retrieved yet. Please do datalad get ... and reload the dataset (File > Load BIDS Dataset...)')
     
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     for key in global_keys:
         if key != "subject" and key != "subjects" and key != "subject_session" and key != "subject_sessions":
             conf_value = config.get('Global', key)
             setattr(pipeline.global_conf, key, conf_value)
-    for stage in pipeline.stages.values():
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+    for stage in list(pipeline.stages.values()):
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             if 'config' in key:  # subconfig
                 sub_config = getattr(stage.config, key)
-                stage_sub_keys = [prop for prop in sub_config.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(sub_config.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     try:
                         conf_value = config.get(stage.name, key + '.' + sub_key)
@@ -383,21 +383,21 @@
 
 
 def dmri_save_config(pipeline, config_path):
-    config = ConfigParser.RawConfigParser()
+    config = configparser.RawConfigParser()
     config.add_section('Global')
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     print(global_keys)
     for key in global_keys:
         # if key != "subject" and key != "subjects":
         config.set('Global', key, getattr(pipeline.global_conf, key))
-    for stage in pipeline.stages.values():
+    for stage in list(pipeline.stages.values()):
         config.add_section(stage.name)
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             keyval = getattr(stage.config, key)
             if 'config' in key:  # subconfig
-                stage_sub_keys = [prop for prop in keyval.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(keyval.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     config.set(stage.name, key + '.' + sub_key, getattr(keyval, sub_key))
             else:
@@ -412,27 +412,27 @@
 
 def dmri_load_config(pipeline, config_path):
     print('>> Load diffusion config file : {}'.format(config_path))
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     
     datalad_is_available = is_tool('datalad')
     try:
         config.read(config_path)
-    except ConfigParser.MissingSectionHeaderError:
+    except configparser.MissingSectionHeaderError:
         print(
             '... error : file is a datalad git annex but it has not been retrieved yet. Please do datalad get ... and reload the dataset (File > Load BIDS Dataset...)')
     
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     for key in global_keys:
         if key != "subject" and key != "subjects" and key != "subject_session" and key != "subject_sessions" and key != 'modalities':
             conf_value = config.get('Global', key)
             setattr(pipeline.global_conf, key, conf_value)
-    for stage in pipeline.stages.values():
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+    for stage in list(pipeline.stages.values()):
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             if 'config' in key:  # subconfig
                 sub_config = getattr(stage.config, key)
-                stage_sub_keys = [prop for prop in sub_config.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(sub_config.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     try:
                         conf_value = config.get(stage.name, key + '.' + sub_key)
@@ -459,20 +459,20 @@
 
 
 def fmri_save_config(pipeline, config_path):
-    config = ConfigParser.RawConfigParser()
+    config = configparser.RawConfigParser()
     config.add_section('Global')
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     for key in global_keys:
         # if key != "subject" and key != "subjects":
         config.set('Global', key, getattr(pipeline.global_conf, key))
-    for stage in pipeline.stages.values():
+    for stage in list(pipeline.stages.values()):
         config.add_section(stage.name)
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             keyval = getattr(stage.config, key)
             if 'config' in key:  # subconfig
-                stage_sub_keys = [prop for prop in keyval.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(keyval.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     config.set(stage.name, key + '.' + sub_key, getattr(keyval, sub_key))
             else:
@@ -487,27 +487,27 @@
 
 def fmri_load_config(pipeline, config_path):
     print('>> Load anatomical config file : {}'.format(config_path))
-    config = ConfigParser.ConfigParser()
+    config = configparser.ConfigParser()
     
     datalad_is_available = is_tool('datalad')
     try:
         config.read(config_path)
-    except ConfigParser.MissingSectionHeaderError:
+    except configparser.MissingSectionHeaderError:
         print(
             '... error : file is a datalad git annex but it has not been retrieved yet. Please do datalad get ... and reload the dataset (File > Load BIDS Dataset...)')
     
-    global_keys = [prop for prop in pipeline.global_conf.traits().keys() if
+    global_keys = [prop for prop in list(pipeline.global_conf.traits().keys()) if
                    not 'trait' in prop]  # possibly dangerous..?
     for key in global_keys:
         if key != "subject" and key != "subjects" and key != "subject_session" and key != "subject_sessions":
             conf_value = config.get('Global', key)
             setattr(pipeline.global_conf, key, conf_value)
-    for stage in pipeline.stages.values():
-        stage_keys = [prop for prop in stage.config.traits().keys() if not 'trait' in prop]  # possibly dangerous..?
+    for stage in list(pipeline.stages.values()):
+        stage_keys = [prop for prop in list(stage.config.traits().keys()) if not 'trait' in prop]  # possibly dangerous..?
         for key in stage_keys:
             if 'config' in key:  # subconfig
                 sub_config = getattr(stage.config, key)
-                stage_sub_keys = [prop for prop in sub_config.traits().keys() if not 'trait' in prop]
+                stage_sub_keys = [prop for prop in list(sub_config.traits().keys()) if not 'trait' in prop]
                 for sub_key in stage_sub_keys:
                     try:
                         conf_value = config.get(stage.name, key + '.' + sub_key)
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/bidsappmanager/pipelines/anatomical/anatomical.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/bidsappmanager/pipelines/anatomical/anatomical.py	(refactored)
@@ -79,7 +79,7 @@
             'Segmentation': SegmentationStageUI(),
             'Parcellation': ParcellationStageUI(pipeline_mode="Diffusion")}
         
-        for stage in self.stages.keys():
+        for stage in list(self.stages.keys()):
             if project_info.subject_session != '':
                 self.stages[stage].stage_dir = os.path.join(self.base_directory, "derivatives", 'nipype', self.subject,
                                                             project_info.subject_session, self.pipeline_name,
@@ -159,7 +159,7 @@
             error(message="Missing required inputs. Please see documentation for more details.", title="Error",
                   buttons=['OK', 'Cancel'], parent=None)
         
-        for stage in self.stages.values():
+        for stage in list(self.stages.values()):
             if stage.enabled:
                 print(stage.name)
                 print(stage.stage_dir)
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/bidsappmanager/pipelines/diffusion/diffusion.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/bidsappmanager/pipelines/diffusion/diffusion.py	(refactored)
@@ -91,7 +91,7 @@
             'Diffusion': DiffusionStageUI(),
             'Connectome': ConnectomeStageUI()}
         
-        for stage in self.stages.keys():
+        for stage in list(self.stages.keys()):
             if project_info.subject_session != '':
                 self.stages[stage].stage_dir = os.path.join(self.base_directory, "derivatives", 'nipype', self.subject,
                                                             project_info.subject_session, self.pipeline_name,
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/bidsappmanager/pipelines/functional/fMRI.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/bidsappmanager/pipelines/functional/fMRI.py	(refactored)
@@ -83,7 +83,7 @@
                        'FunctionalMRI': FunctionalMRIStageUI(),
                        'Connectome': ConnectomeStageUI()}
         
-        for stage in self.stages.keys():
+        for stage in list(self.stages.keys()):
             if project_info.subject_session != '':
                 self.stages[stage].stage_dir = os.path.join(self.base_directory, "derivatives", 'nipype', self.subject,
                                                             project_info.subject_session, self.pipeline_name,
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/pipelines/common.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/pipelines/common.py	(refactored)
@@ -165,7 +165,7 @@
         self.subject = project_info.subject
         self.number_of_cores = project_info.number_of_cores
         
-        for stage in self.stages.keys():
+        for stage in list(self.stages.keys()):
             if project_info.subject_session != '':
                 self.stages[stage].stage_dir = os.path.join(self.base_directory, "derivatives", 'nipype', self.subject,
                                                             project_info.subject_session, self.pipeline_name,
@@ -207,12 +207,12 @@
         return flow
     
     def fill_stages_outputs(self):
-        for stage in self.stages.values():
+        for stage in list(self.stages.values()):
             if stage.enabled:
                 stage.define_inspect_outputs()
     
     def clear_stages_outputs(self):
-        for stage in self.stages.values():
+        for stage in list(self.stages.values()):
             if stage.enabled:
                 stage.inspect_outputs_dict = {}
                 stage.inspect_outputs = ['Outputs not available']
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/pipelines/anatomical/anatomical.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/pipelines/anatomical/anatomical.py	(refactored)
@@ -4,7 +4,7 @@
 #
 #  This software is distributed under the open-source license Modified BSD.
 
-from __future__ import print_function
+
 
 """ Anatomical pipeline Class definition
 """
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/connectome/connectome.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/connectome/connectome.py	(refactored)
@@ -120,7 +120,7 @@
             else:
                 layout = 'matrix'
             
-            if isinstance(mat, basestring):
+            if isinstance(mat, str):
                 # print("is str")
                 if 'gpickle' in mat:
                     # 'Fiber number','Fiber length','Fiber density','ADC','gFA'
@@ -130,7 +130,7 @@
                     # Load the connectivity matrix and extract the attributes (weights)
                     # con_mat =  pickle.load(mat, encoding="latin1")
                     con_mat = nx.read_gpickle(mat)
-                    con_metrics = list(con_mat.edges(data=True))[0][2].keys()
+                    con_metrics = list(list(con_mat.edges(data=True))[0][2].keys())
                     
                     # Create dynamically the list of output connectivity metrics for inspection
                     for con_metric in con_metrics:
@@ -152,7 +152,7 @@
                         # Load the connectivity matrix and extract the attributes (weights)
                         # con_mat =  pickle.load(mat, encoding="latin1")
                         con_mat = nx.read_gpickle(mat)
-                        con_metrics = list(con_mat.edges(data=True))[0][2].keys()
+                        con_metrics = list(list(con_mat.edges(data=True))[0][2].keys())
                         
                         # Create dynamically the list of output connectivity metrics for inspection
                         for con_metric in con_metrics:
@@ -162,7 +162,7 @@
                                                                                         self.config.subject + ' - ' + con_name + ' - ' + metric_str,
                                                                                         map_scale]
             
-            self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in self.inspect_outputs_dict.keys()],
+            self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in list(self.inspect_outputs_dict.keys())],
                                           key=str.lower)
             # print(self.inspect_outputs)
     
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/connectome/fmri_connectome.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/connectome/fmri_connectome.py	(refactored)
@@ -81,7 +81,7 @@
         if (os.path.exists(con_results_path)):
             con_results = pickle.load(gzip.open(con_results_path))
             mat = con_results.outputs.connectivity_matrices
-            if isinstance(mat, basestring):
+            if isinstance(mat, str):
                 print("single scale")
                 # print(mat)
                 if 'gpickle' in mat:
@@ -100,7 +100,7 @@
                             "showmatrix_gpickle", layout, mat, "corr", "False",
                             self.config.subject + ' - ' + con_name + ' - Correlation', map_scale]
             
-            self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in self.inspect_outputs_dict.keys()],
+            self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in list(self.inspect_outputs_dict.keys())],
                                           key=str.lower)
     
     def has_run(self):
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/diffusion/diffusion.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/diffusion/diffusion.py	(refactored)
@@ -23,8 +23,8 @@
 
 # Own imports
 from cmp.stages.common import Stage
-from reconstruction import *
-from tracking import *
+from .reconstruction import *
+from .tracking import *
 from cmtklib.interfaces.misc import ExtractImageVoxelSizes, Tck2Trk
 
 
@@ -658,7 +658,7 @@
             #         FA_results = pickle.load(gzip.open(FA_path))
             #         self.inspect_outputs_dict['MRTrix FA'] = ['mrview',FA_results.outputs.converted]
         
-        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in self.inspect_outputs_dict.keys()],
+        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in list(self.inspect_outputs_dict.keys())],
                                       key=str.lower)
     
     def has_run(self):
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/functional/functionalMRI.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/functional/functionalMRI.py	(refactored)
@@ -212,7 +212,7 @@
                 self.inspect_outputs_dict['Filter output'] = ['fsleyes', '-sdefault', results.outputs.out_file, '-cm',
                                                               'brain_colours_blackbdy_iso']
         
-        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in self.inspect_outputs_dict.keys()],
+        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in list(self.inspect_outputs_dict.keys())],
                                       key=str.lower)
     
     def has_run(self):
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/parcellation/parcellation.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/parcellation/parcellation.py	(refactored)
@@ -288,8 +288,8 @@
                 
                 parc_files = pe.Node(interface=util.IdentityInterface(fields=["roi_colorLUTs", "roi_graphMLs"]),
                                      name="parcellation_files")
-                parc_files.inputs.roi_colorLUTs = [u'{}'.format(p) for p in roi_colorLUTs]
-                parc_files.inputs.roi_graphMLs = [u'{}'.format(p) for p in roi_graphMLs]
+                parc_files.inputs.roi_colorLUTs = ['{}'.format(p) for p in roi_colorLUTs]
+                parc_files.inputs.roi_graphMLs = ['{}'.format(p) for p in roi_graphMLs]
                 
                 # print("^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^")
                 # print("^^^^ Lausanne2008 color LUT / graphML ^^^^")
@@ -340,8 +340,8 @@
                 
                 parc_files = pe.Node(interface=util.IdentityInterface(fields=["roi_colorLUTs", "roi_graphMLs"]),
                                      name="parcellation_files")
-                parc_files.inputs.roi_colorLUTs = [u'{}'.format(p) for p in roi_colorLUTs]
-                parc_files.inputs.roi_graphMLs = [u'{}'.format(p) for p in roi_graphMLs]
+                parc_files.inputs.roi_colorLUTs = ['{}'.format(p) for p in roi_colorLUTs]
+                parc_files.inputs.roi_graphMLs = ['{}'.format(p) for p in roi_graphMLs]
                 
                 flow.connect([
                     (parc_node, outputnode, [("gray_matter_mask_file", "gm_mask_file")]),
@@ -438,7 +438,7 @@
                 # print parc_results
                 # print parc_results.outputs.roi_files_in_structural_space
                 white_matter_file = parc_results.outputs.white_matter_mask_file
-                if isinstance(parc_results.outputs.roi_files_in_structural_space, (str, unicode)):
+                if isinstance(parc_results.outputs.roi_files_in_structural_space, str):
                     # print "str: %s" % parc_results.outputs.roi_files_in_structural_space
                     lut_file = pkg_resources.resource_filename('cmtklib',
                                                                os.path.join('data', 'parcellation', 'nativefreesurfer',
@@ -546,7 +546,7 @@
         else:
             self.inspect_outputs_dict["Custom atlas"] = ['fsleyes', self.config.atlas_nifti_file, "-cm", "random"]
         
-        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in self.inspect_outputs_dict.keys()],
+        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in list(self.inspect_outputs_dict.keys())],
                                       key=str.lower)
     
     def has_run(self):
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/preprocessing/fmri_preprocessing.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/preprocessing/fmri_preprocessing.py	(refactored)
@@ -163,7 +163,7 @@
                                                                        motion_results.outputs.out_file, '-cm',
                                                                        'brain_colours_blackbdy_iso']
         
-        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in self.inspect_outputs_dict.keys()],
+        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in list(self.inspect_outputs_dict.keys())],
                                       key=str.lower)
     
     def has_run(self):
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/preprocessing/preprocessing.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/preprocessing/preprocessing.py	(refactored)
@@ -797,7 +797,7 @@
                     self.inspect_outputs_dict['Eddy current corrected image'] = ['mrview',
                                                                                  eddy_results.outputs.eddy_corrected]
         
-        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in self.inspect_outputs_dict.keys()],
+        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in list(self.inspect_outputs_dict.keys())],
                                       key=str.lower)
     
     def has_run(self):
@@ -833,13 +833,13 @@
         dim = diffusion.shape
         if self.inputs.start > 0 and self.inputs.end > dim[3] - 1:
             print('End volume is set to %d but it should be bellow %d' % (self.inputs.end, dim[3] - 1))
-        padding_idx1 = range(0, self.inputs.start)
+        padding_idx1 = list(range(0, self.inputs.start))
         if len(padding_idx1) > 0:
             temp = diffusion[:, :, :, 0:self.inputs.start]
             nib.save(nib.nifti1.Nifti1Image(temp, affine), os.path.abspath('padding1.nii.gz'))
         temp = diffusion[:, :, :, self.inputs.start:self.inputs.end + 1]
         nib.save(nib.nifti1.Nifti1Image(temp, affine), os.path.abspath('data.nii.gz'))
-        padding_idx2 = range(self.inputs.end, dim[3] - 1)
+        padding_idx2 = list(range(self.inputs.end, dim[3] - 1))
         if len(padding_idx2) > 0:
             temp = diffusion[:, :, :, self.inputs.end + 1:dim[3]]
             nib.save(nib.nifti1.Nifti1Image(temp, affine), os.path.abspath('padding2.nii.gz'))
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/registration/registration.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/registration/registration.py	(refactored)
@@ -1305,7 +1305,7 @@
                     #             else:
                     #                 self.inspect_outputs_dict['Mean-fMRI/%s' % os.path.basename(roi_output)] = ['fsleyes','-sdefault',target.outputs.out_file,roi_output,'-cm','random','-a','50']
         
-        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in self.inspect_outputs_dict.keys()],
+        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in list(self.inspect_outputs_dict.keys())],
                                       key=str.lower)
     
     def has_run(self):
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/segmentation/segmentation.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/stages/segmentation/segmentation.py	(refactored)
@@ -282,7 +282,7 @@
         elif self.config.seg_tool == "Custom segmentation":
             self.inspect_outputs_dict['WM mask'] = ['fsleyes', self.config.white_matter_mask]
         
-        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in self.inspect_outputs_dict.keys()],
+        self.inspect_outputs = sorted([key.encode('ascii', 'ignore') for key in list(self.inspect_outputs_dict.keys())],
                                       key=str.lower)
     
     def has_run(self):
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/viz/reports.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmp/viz/reports.py	(refactored)
@@ -150,7 +150,7 @@
                     
                     chunk_size = 16384
                     
-                    for k, v in crash_info.items():
+                    for k, v in list(crash_info.items()):
                         if k == 'inputs':
                             scope.set_extra(k, dict(v))
                         elif isinstance(v, str) and len(v) > chunk_size:
@@ -175,7 +175,7 @@
                     
                     fingerprint = ''
                     issue_title = node_name + ': ' + gist
-                    for new_fingerprint, error_snippets in fingerprint_dict.items():
+                    for new_fingerprint, error_snippets in list(fingerprint_dict.items()):
                         for error_snippet in error_snippets:
                             if error_snippet in crash_info['traceback']:
                                 fingerprint = new_fingerprint
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmtklib/connectome.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmtklib/connectome.py	(refactored)
@@ -28,8 +28,8 @@
 
 from nipype.utils.filemanip import split_filename
 
-from util import mean_curvature, length
-from parcellation import get_parcellation
+from .util import mean_curvature, length
+from .parcellation import get_parcellation
 
 
 def group_analysis_sconn(output_dir, subjects_to_be_analyzed):
@@ -162,7 +162,7 @@
             resolutions = get_parcellation(parcellation_scheme)
         else:
             resolutions = get_parcellation(parcellation_scheme)
-            for parkey, parval in resolutions.items():
+            for parkey, parval in list(resolutions.items()):
                 for vol, graphml in zip(roi_volumes, roi_graphmls):
                     # print parkey
                     if parkey in vol:
@@ -218,7 +218,7 @@
     # resolution = gconf.parcellation.keys()
     
     streamline_wrote = False
-    for parkey, parval in resolutions.items():
+    for parkey, parval in list(resolutions.items()):
         # if parval['number_of_regions'] != 83:
         #    continue
         
@@ -317,7 +317,7 @@
         mmap = additional_maps
         mmapdata = {}
         print('  >> Maps to be processed :')
-        for k, v in mmap.items():
+        for k, v in list(mmap.items()):
             print("     - %s map" % k)
             da = nibabel.load(v)
             mdata = da.get_data()
@@ -475,7 +475,7 @@
             # and end roi and not going out of the volume
             idx_valid = np.where((fiberlabels[:, 0] == int(u)) & (fiberlabels[:, 1] == int(v)))[0]
             
-            for k, vv in mmapdata.items():
+            for k, vv in list(mmapdata.items()):
                 val = []
                 
                 # print("Processing %s map..." % k)
@@ -487,7 +487,7 @@
                         idx2 = (h[i] / vv[1]).astype(np.uint32)
                         # print "idx2 : ",idx2
                         val.append(vv[0][idx2[:, 0], idx2[:, 1], idx2[:, 2]])
-                    except IndexError, e:
+                    except IndexError as e:
                         print("  ... ERROR - Index error occured when trying extract scalar values for measure", k)
                         print("  ... ERROR - Discard fiber with index", i, "Exception: ", e)
                 
@@ -528,7 +528,7 @@
             # Change w.r.t networkx2
             edge_keys = []
             for u, v, d in G.edges(data=True):
-                edge_keys = d.keys()
+                edge_keys = list(d.keys())
                 break
             
             edge_struct = {}
@@ -541,7 +541,7 @@
             # Get the node attributes/keys from the first node and then break. 
             # Change w.r.t networkx2
             for u, d in G.nodes(data=True):
-                node_keys = d.keys()
+                node_keys = list(d.keys())
                 break
             
             # print "size_nodes : "
@@ -733,7 +733,7 @@
                 resolutions = get_parcellation(self.inputs.parcellation_scheme)
             else:
                 resolutions = get_parcellation(self.inputs.parcellation_scheme)
-                for parkey, parval in resolutions.items():
+                for parkey, parval in list(resolutions.items()):
                     for vol, graphml in zip(self.inputs.roi_volumes, self.inputs.roi_graphmls):
                         print(parkey)
                         if parkey in vol:
@@ -780,7 +780,7 @@
         #     index = np.linspace(0,tp-1,tp).astype('int')
         
         # loop throughout all the resolutions ('scale33', ..., 'scale500')
-        for parkey, parval in resolutions.items():
+        for parkey, parval in list(resolutions.items()):
             print("Resolution = " + parkey)
             
             # Open the corresponding ROI
@@ -810,7 +810,7 @@
         
         ## Apply scrubbing (if enabled) and compute correlation
         # loop throughout all the resolutions ('scale33', ..., 'scale500')
-        for parkey, parval in resolutions.items():
+        for parkey, parval in list(resolutions.items()):
             print("Resolution = " + parkey)
             
             # Open the corresponding ROI
@@ -887,7 +887,7 @@
                 i = -1
                 for i_signal in ts:
                     i += 1
-                    for j in xrange(i, nnodes):
+                    for j in range(i, nnodes):
                         j_signal = ts[j, :]
                         value = np.corrcoef(i_signal, j_signal)[0, 1]
                         G.add_edge(ROI_idx[i], ROI_idx[j])
@@ -902,7 +902,7 @@
                 i = -1
                 for i_signal in ts:
                     i += 1
-                    for j in xrange(i, nnodes):
+                    for j in range(i, nnodes):
                         j_signal = ts[j, :]
                         value = np.corrcoef(i_signal, j_signal)[0, 1]
                         G.add_edge(ROI_idx[i], ROI_idx[j])
@@ -923,7 +923,7 @@
                 # Change w.r.t networkx2
                 edge_keys = []
                 for u, v, d in G.edges(data=True):
-                    edge_keys = d.keys()
+                    edge_keys = list(d.keys())
                     break
                 
                 edge_struct = {}
@@ -936,7 +936,7 @@
                 # Get the node attributes/keys from the first node and then break. 
                 # Change w.r.t networkx2
                 for u, d in G.nodes(data=True):
-                    node_keys = d.keys()
+                    node_keys = list(d.keys())
                     break
                 
                 node_struct = {}
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmtklib/diffusion.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmtklib/diffusion.py	(refactored)
@@ -11,7 +11,7 @@
 import numpy as np
 import nibabel.trackvis as tv
 
-from util import length
+from .util import length
 
 
 def compute_length_array(trkfile=None, streams=None, savefname='lengths.npy'):
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmtklib/functionalMRI.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmtklib/functionalMRI.py	(refactored)
@@ -373,7 +373,7 @@
         # loop throughout all the time points
         FD[0] = 0
         DVARS[0] = 0
-        for i in xrange(1, tp - 1):
+        for i in range(1, tp - 1):
             # FD
             move0 = move[i - 1, :]
             move1 = move[i, :]
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmtklib/parcellation.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmtklib/parcellation.py	(refactored)
@@ -73,7 +73,7 @@
         
         resolutions = get_parcellation(self.inputs.parcellation_scheme)
         
-        for parkey, parval in resolutions.items():
+        for parkey, parval in list(resolutions.items()):
             
             for roi in self.inputs.roi_volumes:
                 if parkey in roi:
@@ -159,7 +159,7 @@
     
     def _gen_outfilenames(self, basename, posfix, parcellation_scheme):
         filepaths = []
-        for scale in get_parcellation(parcellation_scheme).keys():
+        for scale in list(get_parcellation(parcellation_scheme).keys()):
             filepaths.append(op.abspath(basename + '_' + scale + posfix))
         return filepaths
 
@@ -1411,7 +1411,7 @@
     
     def _gen_outfilenames(self, basename, posfix):
         filepaths = []
-        for scale in get_parcellation('Lausanne2018').keys():
+        for scale in list(get_parcellation('Lausanne2018').keys()):
             filepaths.append(op.abspath(basename + '_' + scale + posfix))
         return filepaths
 
@@ -1587,7 +1587,7 @@
             """
             filtered_array = np.copy(array)
             id_regions, num_ids = ndimage.label(filtered_array, structure=struct)
-            id_sizes = np.array(ndimage.sum(array, id_regions, range(num_ids + 1)))
+            id_sizes = np.array(ndimage.sum(array, id_regions, list(range(num_ids + 1))))
             area_mask = (id_sizes == 1)
             filtered_array[area_mask[id_regions]] = 0
             return filtered_array
@@ -1902,7 +1902,7 @@
     
     def _gen_outfilenames(self, basename):
         filepaths = []
-        for scale in get_parcellation(self.inputs.parcellation_scheme).keys():
+        for scale in list(get_parcellation(self.inputs.parcellation_scheme).keys()):
             filepaths.append(op.abspath(basename + '_' + scale + '.nii.gz'))
         return filepaths
 
@@ -2118,7 +2118,7 @@
     
     paths = []
     
-    for scale, features in get_parcellation('Lausanne2008').items():
+    for scale, features in list(get_parcellation('Lausanne2008').items()):
         for hemi in ['lh', 'rh']:
             spath = features['fs_label_subdir_name'] % hemi
             paths.append(spath)
@@ -2225,13 +2225,13 @@
     # LOOP throughout all the SCALES
     # (from the one with the highest number of region to the one with the lowest number of regions)
     # parkeys = gconf.parcellation.keys()
-    scales = get_parcellation('Lausanne2008').keys()
+    scales = list(get_parcellation('Lausanne2008').keys())
     values = list()
     for i in range(len(scales)):
         values.append(get_parcellation('Lausanne2008')[scales[i]]['number_of_regions'])
-    temp = zip(values, scales)
+    temp = list(zip(values, scales))
     temp.sort(reverse=True)
-    values, scales = zip(*temp)
+    values, scales = list(zip(*temp))
     roisMax = np.zeros((256, 256, 256), dtype=np.int16)  # numpy.ndarray
     for i, parkey in enumerate(get_parcellation('Lausanne2008').keys()):
         parval = get_parcellation('Lausanne2008')[parkey]
@@ -2822,7 +2822,7 @@
     
     FNULL = open(os.devnull, 'w')
     
-    for i in reversed(range(0, nscales)):
+    for i in reversed(list(range(0, nscales))):
         
         if v:
             print(' ... working on multiscale parcellation, SCALE {}'.format(i + 1))
@@ -3110,10 +3110,10 @@
     #    fsmask.img(cc_unknown.img==4)    =  1;
     
     # XXX: subtracting wmmask from ROI. necessary?
-    for parkey, parval in get_parcellation('Lausanne2008').items():
+    for parkey, parval in list(get_parcellation('Lausanne2008').items()):
         
         # check if we should subtract the cortical rois from this parcellation
-        if parval.has_key('subtract_from_wm_mask'):
+        if 'subtract_from_wm_mask' in parval:
             if not bool(int(parval['subtract_from_wm_mask'])):
                 continue
         else:
@@ -3139,7 +3139,7 @@
     # remove remaining structure, e.g. brainstem
     gmmask = np.zeros(asegd.shape)
     print("Create gray matter mask")
-    for parkey, parval in get_parcellation('Lausanne2008').items():
+    for parkey, parval in list(get_parcellation('Lausanne2008').items()):
         print("  > Processing %s ..." % ('ROIv_%s.nii.gz' % parkey))
         
         roi = ni.load(op.join(fs_dir, 'label', 'ROIv_%s.nii.gz' % parkey))
@@ -3417,13 +3417,13 @@
     ]
     if parcellation_scheme == 'Lausanne2008':
         ds.append((op.join(fs_dir, 'label', 'cc_unknown.nii.gz'), 'cc_unknown.nii.gz'))
-        for p in get_parcellation('Lausanne2008').keys():
+        for p in list(get_parcellation('Lausanne2008').keys()):
             ds.append((op.join(fs_dir, 'label', 'ROI_%s.nii.gz' % p), 'ROI_Lausanne2008_%s.nii.gz' % p))
             ds.append((op.join(fs_dir, 'label', 'ROIv_%s.nii.gz' % p), 'ROIv_Lausanne2008_%s.nii.gz' % p))
         ds.append((op.join(fs_dir, 'label', 'T1w_class-GM.nii.gz'), 'T1w_class-GM.nii.gz'))
         ds.append((op.join(fs_dir, 'mri', 'aparc+aseg.mgz'), 'aparc+aseg.native.nii.gz'))
     elif parcellation_scheme == 'Lausanne2018':
-        for p in get_parcellation('Lausanne2018').keys():
+        for p in list(get_parcellation('Lausanne2018').keys()):
             # ds.append( (op.join(fs_dir, 'label', 'ROI_%s.nii.gz' % p), 'ROI_HR_th_%s.nii.gz' % p) )
             ds.append((op.join(fs_dir, 'mri', 'ROI_%s_Lausanne2018.nii.gz' % p), 'ROI_Lausanne2018_%s.nii.gz' % p))
             ds.append((op.join(fs_dir, 'mri', 'ROIv_%s_Lausanne2018.nii.gz' % p), 'ROIv_Lausanne2018_%s.nii.gz' % p))
@@ -3517,10 +3517,10 @@
                [79, 13], [80, 26], [81, 17],
                [82, 18], [83, 16]]
     
-    WM = [2, 29, 32, 41, 61, 64, 59, 60, 27, 28] + range(77, 86 + 1) + range(100, 117 + 1) + range(155,
-                                                                                                   158 + 1) + range(195,
-                                                                                                                    196 + 1) + range(
-        199, 200 + 1) + range(203, 204 + 1) + [212, 219, 223] + range(250, 255 + 1)
+    WM = [2, 29, 32, 41, 61, 64, 59, 60, 27, 28] + list(range(77, 86 + 1)) + list(range(100, 117 + 1)) + list(range(155,
+                                                                                                   158 + 1)) + list(range(195,
+                                                                                                                    196 + 1)) + list(range(
+        199, 200 + 1)) + list(range(203, 204 + 1)) + [212, 219, 223] + list(range(250, 255 + 1))
     # add
     # 59  Right-Substancia-Nigra
     # 60  Right-VentralDC
@@ -3545,7 +3545,7 @@
     print("GM mask....")
     # %% create GM parcellation (CORTICAL+SUBCORTICAL)
     # %  -------------------------------------
-    for park in get_parcellation('NativeFreesurfer').keys():
+    for park in list(get_parcellation('NativeFreesurfer').keys()):
         print("Parcellation: " + park)
         GMout = op.join(fs_dir, 'mri', 'ROIv_%s.nii.gz' % park)
         
@@ -3629,7 +3629,7 @@
         (op.join(fs_dir, 'mri', 'aparc+aseg.mgz'), 'aparc+aseg.native.nii.gz')
     ]
     
-    for p in get_parcellation('NativeFreesurfer').keys():
+    for p in list(get_parcellation('NativeFreesurfer').keys()):
         if not op.exists(op.join(fs_dir, 'mri', p)):
             os.makedirs(op.join(fs_dir, 'mri', p))
         ds.append((op.join(fs_dir, 'mri', 'ROIv_%s.nii.gz' % p), op.join(fs_dir, 'mri', p, 'ROIv_HR_th.nii.gz')))
--- /home/localadmin/Softwares/GitHub/connectomemapper3/cmtklib/interfaces/dipy.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/cmtklib/interfaces/dipy.py	(refactored)
@@ -7,7 +7,7 @@
 Interfaces to the algorithms in dipy
 
 """
-from __future__ import print_function, division, unicode_literals, absolute_import
+
 from future import standard_library
 
 standard_library.install_aliases()
@@ -1205,7 +1205,7 @@
         f.close()
         
         # "rtop", "rtap", "rtpp", "msd", "qiv", "ng", "ng_perp", "ng_para"
-        for metric, data in maps.items():
+        for metric, data in list(maps.items()):
             out_name = self._gen_filename(metric)
             nb.Nifti1Image(data, affine).to_filename(out_name)
             IFLOGGER.info('MAP-MRI {metric} image saved as {i}'.format(i=out_name, metric=metric))
--- /home/localadmin/Softwares/GitHub/connectomemapper3/tests/test_parcellation_interfaces.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/tests/test_parcellation_interfaces.py	(refactored)
@@ -151,7 +151,7 @@
         """
         filtered_array = np.copy(array)
         id_regions, num_ids = ndimage.label(filtered_array, structure=struct)
-        id_sizes = np.array(ndimage.sum(array, id_regions, range(num_ids + 1)))
+        id_sizes = np.array(ndimage.sum(array, id_regions, list(range(num_ids + 1))))
         area_mask = (id_sizes == 1)
         filtered_array[area_mask[id_regions]] = 0
         return filtered_array
--- /home/localadmin/Softwares/GitHub/connectomemapper3/scripts/others/create_connectome_report.py	(original)
+++ /home/localadmin/Softwares/GitHub/connectomemapper3/scripts/others/create_connectome_report.py	(refactored)
@@ -28,7 +28,7 @@
 import numpy as np
 import copy
 
-import cStringIO
+import io
 
 try:
     from PIL import Image
@@ -115,7 +115,7 @@
                     imshow(con, interpolation='nearest', norm=colors.LogNorm(), cmap=my_cmap)
                     colorbar()
                     
-                    imgdata = cStringIO.StringIO()
+                    imgdata = io.StringIO()
                     fig.savefig(imgdata, format='png')
                     imgdata.seek(0)  # rewind the data
                     
@@ -148,7 +148,7 @@
                 imshow(con, interpolation='nearest', norm=colors.LogNorm(), cmap=my_cmap)
                 colorbar()
                 
-                imgdata = cStringIO.StringIO()
+                imgdata = io.StringIO()
                 fig.savefig(imgdata, format='png')
                 imgdata.seek(0)  # rewind the data
                 
